{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6f0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc # garbage collector\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1750cec",
   "metadata": {},
   "source": [
    "# Read from S3\n",
    "\n",
    "* To monitor the memory usage, I install the extension `conda install -c conda-forge jupyter-resource-usage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1f1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# s3 = boto3.resource(\n",
    "#     service_name='s3',\n",
    "#     region_name='ap-southeast-2',\n",
    "#     aws_access_key_id='AKIAWBTUBJCDLIOY2ATO',\n",
    "#     aws_secret_access_key='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'\n",
    "# )\n",
    "session = boto3.Session( \n",
    "         aws_access_key_id='AKIAWBTUBJCDLIOY2ATO', \n",
    "         aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902a429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analytics-fy22/2021-07-01.json',\n",
       " 'analytics-fy22/2021-07-02.json',\n",
       " 'analytics-fy22/2021-07-03.json',\n",
       " 'analytics-fy22/2021-07-04.json',\n",
       " 'analytics-fy22/2021-07-05.json',\n",
       " 'analytics-fy22/2021-07-06.json',\n",
       " 'analytics-fy22/2021-07-07.json',\n",
       " 'analytics-fy22/2021-07-08.json',\n",
       " 'analytics-fy22/2021-07-09.json',\n",
       " 'analytics-fy22/2021-07-10.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then use the session to get the resource\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3_bucket_name = 'clear-coles-uts'\n",
    "my_bucket=s3.Bucket(s3_bucket_name)\n",
    "bucket_list = []\n",
    "for objects in my_bucket.objects.filter(Prefix=\"analytics-fy22/\"):\n",
    "#     print(objects.key)\n",
    "    file_name = objects.key\n",
    "    if file_name.find(\".json\")!=-1:\n",
    "        bucket_list.append(objects.key)\n",
    "print(len(bucket_list))\n",
    "bucket_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcafdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bucket_list[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57d988",
   "metadata": {},
   "source": [
    "## Read activities.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28974705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "BUCKET = 'clear-coles-uts'\n",
    "client = client('s3',\n",
    "                 aws_access_key_id='AKIAWBTUBJCDLIOY2ATO',\n",
    "                 aws_secret_access_key='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'\n",
    "                )\n",
    "file = 'analytics-fy22/activities.json'\n",
    "result = client.get_object(Bucket=BUCKET, Key=file) \n",
    "text = result[\"Body\"].read().decode()\n",
    "data=text.split(\"\\n\")#List of the record\n",
    "records = [json.loads(line) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c61cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(records)\n",
    "df.to_csv('activities.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b280b8",
   "metadata": {},
   "source": [
    "## Read all API json files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f5231d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m client(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                  aws_access_key_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAKIAWBTUBJCDLIOY2ATO\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                  aws_secret_access_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNHMR/WOPEXayf6RDywG0+Qgx1kTYQ/msOn8O/tqx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m                 )\n\u001b[1;32m      7\u001b[0m df_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbucket_list\u001b[49m[:\u001b[38;5;241m10\u001b[39m]: \u001b[38;5;66;03m#-1 to read all except activity json\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     FILE_TO_READ \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file:\u001b[39m\u001b[38;5;124m'\u001b[39m,file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket_list' is not defined"
     ]
    }
   ],
   "source": [
    "from boto3 import client\n",
    "BUCKET = 'clear-coles-uts'\n",
    "client = client('s3',\n",
    "                 aws_access_key_id='AKIAWBTUBJCDLIOY2ATO',\n",
    "                 aws_secret_access_key='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'\n",
    "                )\n",
    "df_list=[]\n",
    "for file in bucket_list[:10]: #-1 to read all except activity json\n",
    "    FILE_TO_READ = file\n",
    "    print('Reading file:',file)\n",
    "    result = client.get_object(Bucket=BUCKET, Key=FILE_TO_READ) \n",
    "    text = result[\"Body\"].read().decode()\n",
    "    #Split each row to each record\n",
    "    data=text.split(\"\\n\")#List of the record\n",
    "    records = [json.loads(line) for line in data]\n",
    "    print('The len of {} is :'.format(file),len(records),'\\n')\n",
    "    df_list=df_list+records\n",
    "# Convert it to dataframe for later usage\n",
    "df=pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))\n",
    "print(len(list(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c18db",
   "metadata": {},
   "source": [
    "## Save all files into CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a8fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea08f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.to_csv('full_data_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad44083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read file \n",
    "# df=pd.read_csv('full_data_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f55043",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)#(39631383, 15)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70069bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cols(cell):\n",
    "    \"\"\"This function to handle error when convert string column to dictionary\"\"\"\n",
    "    try:\n",
    "        return literal_eval(cell)\n",
    "    except:\n",
    "        return cell\n",
    "\n",
    "def clean_actorid(df,col_name):\n",
    "    \"\"\"\n",
    "       Single function to clean column actorid \n",
    "       According to the document, the format of this columns is: \"account|https://organisation.clearlrs.com|RANDOM_ID\" \n",
    "    \"\"\"\n",
    "    actor_df = df[col_name].str.split('|', expand=True)\n",
    "    if actor_df.shape[1] > 3:\n",
    "        # Placeholder incase of data issues with this column\n",
    "        df[col_name+'_'+'actor_account', col_name+'_'+'actor_url', col_name+'_'+'actor_uid'] = np.nan\n",
    "    else:    \n",
    "        actor_df.columns = [col_name+'_'+'actor_account', col_name+'_'+'actor_url', col_name+'_'+'actor_uid']\n",
    "        df = pd.concat([df, actor_df], axis=1)    \n",
    "    return df\n",
    "\n",
    "def clean_verb(df,col_name):\n",
    "    \"\"\"\n",
    "       Single function to clean column verb \n",
    "       the format of this columns is like: \" http://adlnet.gov/expapi/verbs/experienced\" \n",
    "       The last section of the url is needed\n",
    "    \"\"\"   \n",
    "    verb_df =df[col_name].str.rsplit(pat='/',n=1,expand=True)\n",
    "    verb_df.columns=[col_name+'_'+'verb_url',col_name+'_'+'verb_verb']\n",
    "    df = pd.concat([df, verb_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def clean_timezone(df):\n",
    "    \"\"\"\n",
    "       Single function to clean column timezone \n",
    "       the format of this columns is like: \"Australia/Adelaide\" \n",
    "    \"\"\"   \n",
    "    tz_df =df['Time Zone'].str.split(pat='/',regex=True,expand=True)\n",
    "    tz_df.columns=['timezone_country','timezone_state']\n",
    "    df = pd.concat([df, tz_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def clean_type(df):\n",
    "    \"\"\"\n",
    "       Single function to clean column type \n",
    "       the format of this columns is like: \"http://adlnet.gov/expapi/activities/module\" \n",
    "    \"\"\"   \n",
    "    type_df =df['type'].str.rsplit(pat='/',n=1,expand=True)\n",
    "    type_df.columns=['type_url','type_type']\n",
    "    df = pd.concat([df, type_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_time(df):\n",
    "    \"\"\"\n",
    "       Convert column timestamp to String and generate other time-related columns by .dt.year/date\n",
    "    \"\"\"       \n",
    "    df['timestamp']=pd.to_datetime(df.loc[:,'timestamp'])\n",
    "    df['date']=df['timestamp'].dt.date\n",
    "#     To be added if reqired\n",
    "#     df['date']=df['timestampe'].dt.time\n",
    "#     df['hour']=df['timestamp'].dt.hour\n",
    "#     df['month']=df['timestamp'].dt.month\n",
    "    return df\n",
    "\n",
    "def clean_activity(df):\n",
    "    \"\"\"\n",
    "       Single function to clean column activity \n",
    "       the format of this columns is like: \"https://organisation.clearlrs.com/activities/INDUCTION_TRAINING\"\n",
    "    \"\"\"       \n",
    "    df['activity_url']=df.loc[:,'activity'].str.split(pat='.com/|.com',regex=True,expand=True)[0]\n",
    "    df['activity_activityid']=df.loc[:,'activity'].str.split(pat='.com/|.com',regex=True,expand=True)[1]\n",
    "    df['activity_activityid']=df['activity_activityid'].str.replace(r'activities/',repl='') #Remove unwanted prefix\n",
    "    return df\n",
    "\n",
    "def get_ancestors_activities(row,depth):\n",
    "    result = []\n",
    "    for i in range(depth+1):\n",
    "        result.append(row[i]['activity'])\n",
    "    return result\n",
    "\n",
    "def get_ancestor_chain(mylist):\n",
    "    result='|'\n",
    "    for element in mylist:\n",
    "        try:\n",
    "            tmp = re.split(pattern='.com/|.com',string=element)\n",
    "            if tmp[1]=='':\n",
    "                result = result+element+'|'\n",
    "            else:\n",
    "                result = result+tmp[1]+'|'\n",
    "        except:\n",
    "#             print('Skipping special activities: {}'.format(element))\n",
    "            result=result+element+'|'\n",
    "    return result    \n",
    "\n",
    "def clean_ancestors(df):\n",
    "    \"\"\"\n",
    "       Single function to clean column ancestors \n",
    "    \"\"\"       \n",
    "    df['ancestor_len']=df.loc[:,'ancestors'].apply(len)-1\n",
    "    df['ancestors_activities']=df.apply(lambda row: get_ancestors_activities(row['ancestors'],row['ancestor_len']),axis='columns')\n",
    "    df['ancestor_chain']=df.apply(lambda row: get_ancestor_chain(row['ancestors_activities']),axis='columns')\n",
    "    return df\n",
    "\n",
    "def extract_other_value(row):\n",
    "    \"\"\"\n",
    "       Single function to clean column `other` \n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = row['other'][0]['activity']\n",
    "    except:\n",
    "        result = None\n",
    "    return result\n",
    "    \n",
    "def clean_other(df):\n",
    "    df['other_extracted']=df.apply(lambda row: extract_other_value(row),axis='columns')\n",
    "    return df\n",
    "\n",
    "def extract_contained_value(row):\n",
    "    \"\"\"\n",
    "       Single function to clean column `contained` \n",
    "    \"\"\"\n",
    "    try:\n",
    "        tmp = row['contained']\n",
    "#         tmp = tmp[t.index.values[0]] \n",
    "        result=tmp['actorId']+';'+tmp['verb']\n",
    "    except:\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "def clean_contain(df):\n",
    "    df['contained_extracted']=df.apply(lambda row: extract_contained_value(row),axis='columns')\n",
    "#     df['contained_extracted']=df['contained_extracted'].str.split(pat=';')\n",
    "    df_contrained_list = df['contained_extracted'].str.split(pat=';',expand=True)\n",
    "    if df_contrained_list.shape[1] <2:\n",
    "        # Placeholder incase of data issues with this column\n",
    "        df['contained_acc', 'contained_verb'] = np.nan\n",
    "    else:    \n",
    "        df_contrained_list.columns = ['contained_acc', 'contained_verb']\n",
    "        df = pd.concat([df, df_contrained_list], axis=1)\n",
    "    return df\n",
    "\n",
    "def drop_col(df,cols):\n",
    "    df.drop(cols,axis=1,inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67651b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ancestors']=df['ancestors'].apply(eval_cols)\n",
    "df['other']=df['other'].apply(eval_cols)\n",
    "df['contained']=df['contained'].apply(eval_cols)\n",
    "# df_new=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.pipe(clean_actorid,'actorId').pipe(clean_type).pipe(clean_timezone).pipe(clean_verb,'verb').pipe(get_time)\\\n",
    ".pipe(clean_activity).pipe(clean_ancestors).pipe(clean_other).pipe(clean_contain).pipe(clean_actorid,'contained_acc')\\\n",
    ".pipe(clean_verb,'contained_verb').pipe(drop_col,['actorId','verb','type','activity','ancestors','Time Zone','other','contained','contained_extracted','ancestors_activities','contained_acc','contained_verb'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('transformed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e045e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=drop_col(df,['timezone_country','actorId_actor_account','actorId_actor_url','activity_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde47b85-6491-4ef2-9b2b-dff2aa0b712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names to lower case and underscores instead of spaces \n",
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d79c61",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a016e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c565634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['type_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cecf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['verb_verb_verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c990ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ancestor_chain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8535e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contained_verb_verb_verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2fc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deriving manager assigned activities \n",
    "\n",
    "# Step 1 - create a new column for activity_activityid + actor_id called activity_actor \n",
    "df['activity_actor'] = df['activity_activityid'] + df['actorid_actor_uid']\n",
    "\n",
    "# Step 2 - create a new column for activity_activity + contained_actor_id called assigned_activity_actor\n",
    "df['assigned_activity_actor'] = df['activity_activityid'] + df['contained_acc_actor_uid']\n",
    "\n",
    "# Step 3 - create a new dataframe for the associated verbs  \n",
    "associated_data = df[['verb_verb_verb','assigned_activity_actor']].copy()\n",
    "associated_data = associated_data.dropna()\n",
    "\n",
    "# create columns with a count of each time the verb occurs for each assigned_activity_actor \n",
    "# https://stackoverflow.com/questions/41951223/in-pandas-how-to-perform-value-counts-of-genderor-any-categorical-variable-ba\n",
    "associated_data_count = associated_data.groupby(by = ['assigned_activity_actor', 'verb_verb_verb']).size().unstack(fill_value=0)\n",
    "\n",
    "# convert the row names (assigned_activity_actor values) back into a column called 'activity_actor' for merging in Step 4\n",
    "associated_data_count.index.name = 'activity_actor'\n",
    "associated_data_count.reset_index(inplace=True)\n",
    "\n",
    "# check info\n",
    "# print(associated_data_count.info())\n",
    "\n",
    "# save to csv \n",
    "# associated_data_count.to_csv(r'/Volumes/GoogleDrive/My Drive/Uni/iLab2/Data/associated_data_count.csv', index = False)\n",
    "\n",
    "# Step 4 - join the associated_data_count dataframe to the 'data' dataframe \n",
    "# we can now see which activities were 'assigned','removed', or shared by someone else (i.e. a manager)\n",
    "merged_df = pd.merge(df, associated_data_count, how = 'left', on = 'activity_actor')\n",
    "merged_df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcfc3c-9fb2-42d8-91a8-495d40542aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace na with 0 for assigned, removed and shared columns \n",
    "merged_df['assigned'].fillna(0, inplace=True)\n",
    "merged_df['removed'].fillna(0, inplace=True)\n",
    "merged_df['shared'].fillna(0, inplace=True)\n",
    "merged_df['other_extracted'].fillna('blank', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b5afc-748a-4b29-b784-20611d5256d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving 'admin assigned' activities \n",
    "# Step 1 - where verb_verb = registered and other = null, this is admin-assigned \n",
    "merged_df['admin_assigned_activity_actor_id'] = np.where((merged_df.verb_verb_verb.str.contains('registered', na=False)) \n",
    "                                                         & (merged_df.other_extracted.str.contains('blank', na=False)), \n",
    "                                                         merged_df['activity_actor'], 0) \n",
    "\n",
    "# Step 2 - where 'admin_assigned_activity_actor_id' = 'activity_actor' then 'admin_assigned' = 1\n",
    "# note: observations where the activity is being assigned to the user i.e. where verb_verb = registered and other = null, are not counted as a user doing an \"admin assigned\" activity. i.e. Where verb_verb = registered, these are considered as 0 \n",
    "merged_df['admin_assigned'] = np.where((merged_df['activity_actor'].isin(merged_df['admin_assigned_activity_actor_id']))\n",
    "                                       & (~merged_df.verb_verb_verb.str.contains('registered', na=False)), \n",
    "                                          1, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15be51-d124-4e63-bdd0-998526e7a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby(['other_extracted','verb_verb_verb'])['verb_verb_verb'].count().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc538c1-cef1-45a1-8b89-25015e181b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving 'user assigned' activities \n",
    "# Step 1 - where verb_verb = registered and other = not null, this is self-assigned \n",
    "merged_df['user_assigned_activity_actor_id'] = np.where((merged_df.verb_verb_verb.str.contains('registered', na=False)) \n",
    "                                      & (merged_df['other_extracted'].notnull()), \n",
    "                                      merged_df['activity_actor'], 0) \n",
    "\n",
    "# Step 2 - where 'admin_assigned_activity_actor_id' = 'activity_actor' then 'admin_assigned' = 1\n",
    "# note: observations where the user is registering for the activity i.e. where verb_verb = registered and other = not null, are not counted as a user doing a \"user assigned\" activity. i.e. Where verb_verb = registered, these are considered as 0 \n",
    "merged_df['user_assigned'] = np.where((merged_df['activity_actor'].isin(merged_df['user_assigned_activity_actor_id']))\n",
    "                                       & (~merged_df.verb_verb_verb.str.contains('registered', na=False)), \n",
    "                                          1, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143370cb-0967-40dc-bf5b-da9f091199f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving 'manager assigned' activities - make them easier to count when we group the dataframe\n",
    "# Step 1 - where assigned > 0 then manager_assigned = 1 \n",
    "merged_df['manager_assigned'] = np.where((merged_df.assigned > 0), 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7912e9-39e8-4d11-a262-bbe31ab9e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for total non-user assigned activities \n",
    "merged_df['total_non_user_assigned'] = merged_df['manager_assigned'] + merged_df['admin_assigned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1bc99-7411-41f9-871c-6f0eb65b9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of completions \n",
    "# create a new column that is True (1) or False (0) where a row is a manager_assigned completion or not \n",
    "merged_df['manager_assigned_completion'] = np.where((merged_df.verb_verb_verb.str.contains('complete', na=False)) \n",
    "                                            & (merged_df['manager_assigned'] > 0), 1, 0)  \n",
    "\n",
    "# create a new column that is True (1) or False (0) where a row is an admin_assigned completion or not \n",
    "merged_df['admin_assigned_completion'] = np.where((merged_df.verb_verb_verb.str.contains('complete', na=False)) \n",
    "                                            & (merged_df['admin_assigned'] > 0), 1, 0)  \n",
    "\n",
    "# create a new column that is True (1) or False (0) where a row is a user_assigned completion or not \n",
    "merged_df['user_assigned_completion'] = np.where((merged_df.verb_verb_verb.str.contains('complete', na=False)) \n",
    "                                               & (merged_df['user_assigned'] > 0), 1, 0)  \n",
    "\n",
    "# create a new column that is True (1) or False (0) where a row is a non_user_assigned completion or not \n",
    "merged_df['total_non_user_assigned_completion'] = np.where((merged_df.verb_verb_verb.str.contains('complete', na=False)) \n",
    "                                               & (merged_df['total_non_user_assigned'] > 0), 1, 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0e183-888e-400d-b22f-08fd02bf3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns that we don't need for analysis \n",
    "merged_df = merged_df.drop(columns=['activity_actor', 'assigned_activity_actor', 'admin_assigned_activity_actor_id', 'user_assigned_activity_actor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfa590-f2fe-4dd7-91f0-85c69253456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22560a2a-4d0d-4424-bdb0-a48d2280fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv \n",
    "merged_df.to_csv(r'/Volumes/GoogleDrive/My Drive/Uni/iLab2/Data/tian_merged_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e85b1-cef5-4140-8905-4d89041aa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables \n",
    "# 1. Count of Manager-Assigned activities per Actor Id \n",
    "# 2. Count of Admin-Assigned activities per Actor Id \n",
    "# 3. Count of User-Assigned activities per Actor Id \n",
    "# 4. Count of Non-User-Assigned activities per Actor Id \n",
    "# 5. Count of Manager-Assigned completions \n",
    "# 6. Count of Admin-Assigned completions\n",
    "# 7. Count of User-Assigned completions\n",
    "# 8. Count of Non-User-Assigned completions\n",
    "\n",
    "\n",
    "cols = ['activity_activityid', 'duration', 'completion', 'is_manager', 'shared', 'admin_assigned', 'user_assigned', 'manager_assigned', 'total_non_user_assigned', 'manager_assigned_completion', 'admin_assigned_completion', 'user_assigned_completion', 'total_non_user_assigned_completion']\n",
    "\n",
    "actor_df = merged_df.groupby('actorid_actor_uid')[cols].agg({'activity_activityid': 'count',\n",
    "                                                    'duration': 'sum',\n",
    "                                                    'completion': 'sum',\n",
    "                                                    'is_manager': 'last',\n",
    "                                                    'shared': 'sum', \n",
    "                                                    'admin_assigned': 'sum', \n",
    "                                                    'user_assigned': 'sum', \n",
    "                                                    'manager_assigned': 'sum', \n",
    "                                                    'total_non_user_assigned': 'sum', \n",
    "                                                    'manager_assigned_completion': 'sum', \n",
    "                                                    'admin_assigned_completion': 'sum', \n",
    "                                                    'user_assigned_completion': 'sum', \n",
    "                                                    'total_non_user_assigned_completion': 'sum'\n",
    "                                                   }).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b7fcc-924e-4b1d-b89e-26e6254a695d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88066980-124a-4854-a73f-e2124e85dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables \n",
    "# 1. Avg monthly number of Manager-Assigned activities per Actor Id \n",
    "# 2. Avg monthly number of Admin-Assigned activities per Actor Id \n",
    "# 3. Avg monthly number of User-Assigned activities per Actor Id \n",
    "# 4. Avg monthly number of Non-User-Assigned activities per Actor Id \n",
    "\n",
    "# Step 1 - create a data frame with the monthly counts \n",
    "merged_df['month'] = pd.to_datetime(merged_df['date']).dt.month\n",
    "month_df = merged_df.groupby(['month', 'actorid_actor_uid']).agg({'date': 'max',\n",
    "                                                 'activity_activityid': 'count',\n",
    "                                                 'duration': 'sum',\n",
    "                                                 'completion': 'sum',\n",
    "                                                 'is_manager': 'last',\n",
    "                                                 'admin_assigned': 'sum', \n",
    "                                                 'user_assigned': 'sum', \n",
    "                                                 'manager_assigned': 'sum', \n",
    "                                                 'total_non_user_assigned': 'sum', \n",
    "                                                 'manager_assigned_completion': 'sum', \n",
    "                                                 'admin_assigned_completion': 'sum', \n",
    "                                                 'user_assigned_completion': 'sum', \n",
    "                                                 'total_non_user_assigned_completion': 'sum'\n",
    "                                                       }).reset_index()\n",
    "\n",
    "# Step 2 - create a dataframe with the average monthly number of user-assigned, manager-assigned and admin-assigned activities \n",
    "avg_month_df = month_df.groupby(['actorid_actor_uid']).agg({'admin_assigned': 'mean', \n",
    "                                                 'user_assigned': 'mean', \n",
    "                                                 'manager_assigned': 'mean', \n",
    "                                                 'total_non_user_assigned': 'mean', \n",
    "                                                 'manager_assigned_completion': 'mean', \n",
    "                                                 'admin_assigned_completion': 'mean', \n",
    "                                                 'user_assigned_completion': 'mean', \n",
    "                                                 'total_non_user_assigned_completion': 'mean'\n",
    "                                                       }).reset_index()\n",
    "\n",
    "# Step 3 - rename the columns in the avg_month_df \n",
    "avg_month_df.rename(columns={'admin_assigned': 'avg_mth_no_admin_assigned', \n",
    "                             'user_assigned': 'avg_mth_no_user_assigned', \n",
    "                             'manager_assigned': 'avg_mth_no_manager_assigned',\n",
    "                             'total_non_user_assigned': 'avg_mth_no_non_user_assigned',\n",
    "                             'manager_assigned_completion': 'avg_mth_no_manager_assigned_completion',\n",
    "                             'admin_assigned_completion': 'avg_mth_no_admin_assigned_completion',\n",
    "                             'user_assigned_completion': 'avg_mth_no_user_assigned_completion',\n",
    "                             'total_non_user_assigned_completion': 'avg_mth_no_non_user_assigned_completion'\n",
    "                            }, inplace=True)\n",
    "\n",
    "# Step 4 - bind the monthly averages to the actor_df dataframe \n",
    "grouped_df = pd.merge(actor_df, avg_month_df, how = 'left', on = 'actorid_actor_uid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231fa03-20bf-4173-9740-8aec2bc111d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_csv(r'/Volumes/GoogleDrive/My Drive/Uni/iLab2/Data/grouped_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40c548-69ef-4bcc-898d-19cc6b208de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27989651",
   "metadata": {},
   "source": [
    "### Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df=df.sample(frac=0.1)\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.groupby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e7646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
